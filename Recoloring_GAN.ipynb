{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras.layers import LeakyReLU, BatchNormalization, Input, Activation, Concatenate\n",
    "import numpy as np\n",
    "from keras.initializers import RandomNormal\n",
    "from numpy.random import randint\n",
    "from matplotlib import pyplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs_shape = (32,32,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Discriminator\n",
    "\n",
    "def discriminator():\n",
    "  init = RandomNormal(stddev=0.02)\n",
    "  in_src_image = Input(shape=imgs_shape)\n",
    "  in_target_image = Input(shape=imgs_shape)\n",
    "  merged = Concatenate()([in_src_image, in_target_image])\n",
    "\n",
    "  d = tf.keras.layers.Conv2D(32, (4,4), strides=(2,2), padding='same', kernel_initializer=init)(merged)\n",
    "  d = LeakyReLU(alpha=0.2)(d)\n",
    "  \n",
    "  d = tf.keras.layers.Conv2D(64, (4,4), strides=(2,2), padding='same', kernel_initializer=init)(d)\n",
    "  d = BatchNormalization()(d)\n",
    "  d = LeakyReLU(alpha=0.2)(d)\n",
    "\n",
    "  d = tf.keras.layers.Conv2D(128, (4,4), strides=(2,2), padding='same', kernel_initializer=init)(d)\n",
    "  d = BatchNormalization()(d)\n",
    "  d = LeakyReLU(alpha=0.2)(d)\n",
    "\n",
    "  d = tf.keras.layers.Conv2D(256, (4,4), strides=(2,2), padding='same', kernel_initializer=init)(d)\n",
    "  d = BatchNormalization()(d)\n",
    "  d = LeakyReLU(alpha=0.2)(d)\n",
    "\n",
    "  d = tf.keras.layers.Conv2D(1, (4,4), padding='same', kernel_initializer=init)(d)\n",
    "  patch_out = Activation('sigmoid')(d)\n",
    "  \n",
    "  model = keras.models.Model([in_src_image, in_target_image], patch_out)\n",
    "\n",
    "  opt = keras.optimizers.Adam(learning_rate=0.0002, beta_1=0.5)\n",
    "  model.compile(loss='binary_crossentropy', optimizer=opt, loss_weights=[0.5])\n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generator\n",
    "\n",
    "def generator():\n",
    "    X = Input(shape = imgs_shape)\n",
    "  \n",
    "    conv1 = tf.keras.layers.Conv2D( 32 , kernel_size=( 5 , 5 ) , strides=1 )( X )\n",
    "    conv1 = tf.keras.layers.LeakyReLU()( conv1 )\n",
    "    conv1 = tf.keras.layers.Conv2D( 64 , kernel_size=( 3 , 3 ) , strides=1)( conv1 )\n",
    "    conv1 = tf.keras.layers.LeakyReLU()( conv1 )\n",
    "\n",
    "    conv2 = tf.keras.layers.Conv2D( 64 , kernel_size=( 5 , 5 ) , strides=1)( conv1 )\n",
    "    conv2 = tf.keras.layers.LeakyReLU()( conv2 )\n",
    "    conv2 = tf.keras.layers.Conv2D( 128 , kernel_size=( 3 , 3 ) , strides=1 )( conv2 )\n",
    "    conv2 = tf.keras.layers.LeakyReLU()( conv2 )\n",
    "\n",
    "    conv3 = tf.keras.layers.Conv2D( 128 , kernel_size=( 5 , 5 ) , strides=1 )( conv2 )\n",
    "    conv3 = tf.keras.layers.LeakyReLU()( conv3 )\n",
    "    conv3 = tf.keras.layers.Conv2D( 256 , kernel_size=( 3 , 3 ) , strides=1 )( conv3 )\n",
    "    conv3 = tf.keras.layers.LeakyReLU()( conv3 )\n",
    "\n",
    "    bottleneck = tf.keras.layers.Conv2D( 256 , kernel_size=( 3 , 3 ) , strides=1 , activation='tanh' , padding='same' )( conv3 )\n",
    "\n",
    "    concat_1 = tf.keras.layers.Concatenate()( [ bottleneck , conv3 ] )\n",
    "    conv_up_3 = tf.keras.layers.Conv2DTranspose( 128 , kernel_size=( 3 , 3 ) , strides=1 , activation='relu' )( concat_1 )\n",
    "    conv_up_3 = tf.keras.layers.Conv2DTranspose( 64 , kernel_size=( 5 , 5 ) , strides=1 , activation='relu' )( conv_up_3 )\n",
    "\n",
    "    concat_2 = tf.keras.layers.Concatenate()( [ conv_up_3 , conv2 ] )\n",
    "    conv_up_2 = tf.keras.layers.Conv2DTranspose( 64 , kernel_size=( 3 , 3 ) , strides=1 , activation='relu' )( concat_2 )\n",
    "    conv_up_2 = tf.keras.layers.Conv2DTranspose( 32 , kernel_size=( 5 , 5 ) , strides=1 , activation='relu' )( conv_up_2 )\n",
    "\n",
    "    concat_3 = tf.keras.layers.Concatenate()( [ conv_up_2 , conv1 ] )\n",
    "    conv_up_1 = tf.keras.layers.Conv2DTranspose( 32 , kernel_size=( 3 , 3 ) , strides=1 , activation='relu')( concat_3 )\n",
    "    conv_up_1 = tf.keras.layers.Conv2DTranspose( 3 , kernel_size=( 5 , 5 ) , strides=1 , activation='relu')( conv_up_1 )\n",
    "  \n",
    "    model = tf.keras.models.Model( X , conv_up_1 )\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GAN model, combined discriminator and Generator\n",
    "def gan_model(generator, discriminator, input_img):\n",
    "  discriminator.trainable = False\n",
    "  src_input = keras.layers.Input(shape = input_img)\n",
    "  gen_output = generator(src_input)\n",
    "  disc_output = discriminator([src_input, gen_output])\n",
    "  model = keras.models.Model(inputs=src_input, outputs=[disc_output, gen_output])\n",
    "  opt = keras.optimizers.Adam(learning_rate=0.0002, beta_1=0.5)\n",
    "  model.compile(loss=[\"binary_crossentropy\", \"mae\"], optimizer=opt, loss_weights=[1,100])\n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(filename):\n",
    "  data = np.load(filename)\n",
    "  X1, X2 = data[\"arr_0\"], data[\"arr_1\"]\n",
    "  X1 = (X1 - 127.5)/127.5\n",
    "  X2 = (X2 - 127.5)/127.5\n",
    "  return [X1,X2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_real_images(dataset, n, patch_shape):\n",
    "  trainA, trainB = dataset\n",
    "  ix = randint(0, trainA.shape[0],n)\n",
    "  X1, X2 = trainA[ix], trainB[ix]\n",
    "  y = np.ones((n, patch_shape, patch_shape,1))\n",
    "  return [X1,X2], y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_fake_images(model, sample, patch_shape):\n",
    "  X = model.predict(sample)\n",
    "  y = np.zeros((len(X), patch_shape, patch_shape, 1))\n",
    "  return X,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def performance_check(step, model, dataset, n=3):\n",
    "  [realA, realB], _ = generate_real_images(dataset, n, 1)\n",
    "  fakeB, _ = generate_fake_images(model, realA, 1)\n",
    "  realA = (realA+1)/2.0\n",
    "  realB = (realB+1)/2.0\n",
    "  fakeB = (fakeB+1)/2.0\n",
    "\n",
    "# plot real source images\n",
    "  for i in range(n):\n",
    "    pyplot.subplot(3, n, 1+i)\n",
    "    pyplot.axis(\"off\")\n",
    "    pyplot.imshow(realA[i])\n",
    "  \n",
    "#plot generated target images  \n",
    "  for i in range(n):\n",
    "    pyplot.subplot(3, n, 1+n+i)\n",
    "    pyplot.axis(\"off\")\n",
    "    pyplot.imshow(fakeB[i])\n",
    "\n",
    "# plot real target images\n",
    "  for i in range(n):\n",
    "    pyplot.subplot(3, n, 1+n*2+i)\n",
    "    pyplot.axis(\"off\")\n",
    "    pyplot.imshow(realB[i])\n",
    "  filename1 = 'plot_%06d.png' % (step+1)\n",
    "  pyplot.savefig(filename1)\n",
    "  pyplot.close()\n",
    "  filename2 = 'model_%6d.h5' % (step+1)\n",
    "  model.save_weights(filename2)\n",
    "  print(\">saved: %s and %s\" % (filename1, filename2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GAN training function\n",
    "\n",
    "def train_gan(disc_model, gen_model, gan_model, dataset, epochs=150, batch=64):\n",
    "  n_patch = disc_model.output_shape[1]\n",
    "  trainA, trainB = dataset\n",
    "  batch_per_epoch = int(len(trainA)/batch)  #400\n",
    "  print(batch_per_epoch)\n",
    "  steps = batch_per_epoch*epochs   #40000\n",
    "  \n",
    "  for i in range(steps):\n",
    "    print(i+1)\n",
    "    [realA, realB], y_real = generate_real_images(dataset, batch, n_patch)\n",
    "    fakeB, y_fake = generate_fake_images(gen_model, realA, n_patch)\n",
    "    disc_loss1 = disc_model.train_on_batch([realA, realB], y_real)\n",
    "    disc_loss2 = disc_model.train_on_batch([realA, fakeB], y_fake)\n",
    "    gen_loss, _, _ = gan_model.train_on_batch(realA, [y_real, realB])\n",
    "    print(\">%d, d1[%.3f] d2[%.3f] g[%.3f]\" %(i+1, disc_loss1, disc_loss2, gen_loss))  \n",
    "    if (i+1) % (batch_per_epoch*5) == 0 :  \n",
    "      performance_check(i, gen_model, dataset)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading dataset\n",
    "\n",
    "dataset = load_dataset('dataset\\cifar10.npz')\n",
    "print(\"Loaded\", dataset[0].shape, dataset[1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_shape = dataset[0].shape[1:]\n",
    "disc_model = discriminator()\n",
    "gen_model = generator() #generator(image_shape)\n",
    "gan_model = gan_model(gen_model, disc_model, image_shape)\n",
    "train_gan(disc_model, gen_model, gan_model, dataset)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
